Save the following markdown content into a file at .windsurf/workflows/implement-architecture.md. Once saved, you can simply type /implement-architecture in the Cascade chat, and the AI will systematically execute the upgrades.Internal Code Brain: Architecture Implementation WorkflowDescription: Guides the AI agent through implementing the enterprise performance, concurrency, and retrieval optimizations for the local Code Brain MCP server.InstructionsCascade, please execute the following implementation phases sequentially. Do not proceed to the next phase until the current one is complete, tested, and approved by the user.Phase 1: Database Concurrency & OptimizationEnable WAL Mode: Open src/lib/db.ts and ensure the SQLite connection executes PRAGMA journal_mode=WAL; upon initialization to allow concurrent reading and writing without locking.Binary Quantization: Modify the vector insertion logic in src/ingestion/sync_handler.ts and the schema in db.ts. Utilize the vec_quantize_binary() scalar function to convert float32 embeddings into bitvectors for lower memory footprint.Micro-Batching: Implement an in-memory queue/ring-buffer in src/lib/worker_pool.ts or sync_handler.ts to aggregate incoming vector inserts. Flush this queue within a single SQLite transaction every 500ms or when the buffer hits 1,000 items.Phase 2: Semantic Chunking (cAST)Integrate Tree-sitter: Open src/workers/parser.worker.ts. Discard any naive token-counting logic.Implement Split-then-Merge: Construct the cAST algorithm. Traverse the AST top-down, keeping function signatures, docstrings, and classes intact. If a node exceeds the maximum chunk size, recursively split it, then greedily merge smaller adjacent sibling nodes into single chunks.Katz Centrality Prep: Update the graph extraction task to count incoming/outgoing edges (calls/imports) so we can later calculate Katz centrality to score node importance.Phase 3: Hybrid Search & Cross-Encoder RerankingRefine RRF: Open src/lib/hybrid_search.ts. Ensure the Reciprocal Rank Fusion (RRF) merging the sqlite-vec and FTS5 results strictly uses the constant parameter k=60.Cross-Encoder Integration: Modify the reranking stage. Extract the top 100 hits from the RRF output. Instead of bi-encoder cosine similarity, pass the concatenated string (user_query + code_chunk) into a Cross-Encoder model to score their nuanced structural relevance.Phase 4: MCP Standards & ScalabilityResource vs. Tool Delineation: Review src/server/mcp_server.ts. Ensure exact codebase searches are actionable Tools, while raw file contents and massive repository graphs are exposed as read-only Resources using specific URIs to prevent context window bloat.Opaque Cursor Pagination: Update any Tools or Resources returning large arrays. Implement opaque, cursor-based pagination (e.g., returning a base64 encoded position string as nextCursor) so the LLM processes massive result sets iteratively.Streaming: Ensure StdioServerTransport uses Node.js streams (with a chunk size limit) to handle multi-megabyte file reads.Phase 5: Hardware Inference TuningContinuous Batching: Update the node-llama-cpp configuration in src/workers/ai.worker.ts and llm.worker.ts. Enable continuous batching (-cb) and aggressively over-provision the context window (e.g., setting -c 8192) to prevent KV cache fragmentation.Split Mode Graph: If multi-GPU is detected, ensure the inference engine uses split mode graph execution rather than layer splitting to maximize parallel compute density.Why this Workflow is EffectiveStructured Execution: By breaking the prompt into phases, it prevents the LLM from attempting to rewrite the entire application at once, which usually results in truncated code or broken logic.Built-in Best Practices: The workflow inherently instructs Cascade to use the mathematically optimal $k=60$ constant for Reciprocal Rank Fusion  and the specific vec_quantize_binary() function for sqlite-vec.Adherence to MCP Specifications: It explicitly enforces opaque cursor pagination (using nextCursor) and strict URI delineation for Resources, ensuring the final output perfectly aligns with the official Model Context Protocol standards.Codebase Awareness: It points the agent directly to the relevant files in your specific project structure (e.g., src/lib/db.ts or src/lib/hybrid_search.ts), drastically speeding up the execution time.
